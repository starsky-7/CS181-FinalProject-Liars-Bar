"""
å¿«é€Ÿå¯åŠ¨ä¸åŒè®­ç»ƒé…ç½®çš„è„šæœ¬
"""

from advanced_trainer import AdvancedRLTrainer


def quick_test():
    """å¿«é€Ÿæµ‹è¯•ï¼ˆ100å±€ï¼‰"""
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•é…ç½®")
    
    config = {
        "learning_rate": 0.001,
        "discount_factor": 0.95,
        "epsilon": 1.0,
        "epsilon_decay": 0.99,
        "epsilon_min": 0.1,
    }
    
    trainer = AdvancedRLTrainer("dqn", config, "quick_test")
    trainer.train(
        num_episodes=100,
        opponent_types=["simple", "simple", "simple"],
        eval_interval=20,
        eval_games=10,
        save_interval=50,
        plot_interval=20,
        shuffle_positions=True  # å¯ç”¨ä½ç½®æ‰“ä¹±
    )


def medium_training():
    """ä¸­ç­‰è§„æ¨¡è®­ç»ƒï¼ˆ1000å±€ï¼‰"""
    print("ğŸ¯ ä¸­ç­‰è§„æ¨¡è®­ç»ƒ")
    
    config = {
        "learning_rate": 0.001,
        "discount_factor": 0.95,
        "epsilon": 1.0,
        "epsilon_decay": 0.998,
        "epsilon_min": 0.05,
    }
    
    trainer = AdvancedRLTrainer("dqn", config, "medium_training")
    trainer.train(
        num_episodes=1000,
        opponent_types=["humanlike", "simple", "simple"],
        eval_interval=50,
        eval_games=20,
        save_interval=200,
        plot_interval=50,
        shuffle_positions=True  # å¯ç”¨ä½ç½®æ‰“ä¹±
    )


def long_training():
    """é•¿æ—¶é—´è®­ç»ƒï¼ˆ5000å±€ï¼‰"""
    print("ğŸ”¥ é•¿æ—¶é—´è®­ç»ƒ")
    
    config = {
        "learning_rate": 0.001,
        "discount_factor": 0.95,
        "epsilon": 1.0,
        "epsilon_decay": 0.9995,
        "epsilon_min": 0.05,
        "batch_size": 64,
        "warmup_steps": 500,
    }
    
    trainer = AdvancedRLTrainer("dqn", config, "long_training")
    trainer.train(
        num_episodes=5000,
        opponent_types=["humanlike", "simple", "simple"],
        eval_interval=100,
        eval_games=30,
        save_interval=500,
        plot_interval=100,
        shuffle_positions=True  # å¯ç”¨ä½ç½®æ‰“ä¹±
    )


def baseline_comparison():
    """åŸºçº¿å¯¹æ¯”ï¼šä¸ä½¿ç”¨å¯¹æ‰‹ç‰¹å¾"""
    print("ğŸ“Š åŸºçº¿å¯¹æ¯”å®éªŒ")
    
    from rl_trainer import RLTrainer
    
    config = {
        "learning_rate": 0.001,
        "discount_factor": 0.95,
        "epsilon": 1.0,
        "epsilon_decay": 0.998,
        "epsilon_min": 0.05,
    }
    
    # æ³¨æ„ï¼šä½¿ç”¨æ™®é€šRLTrainerï¼ˆå¸¦å¯¹æ‰‹ç‰¹å¾ï¼‰
    trainer = RLTrainer("dqn", config)
    
    print("\nè®­ç»ƒ1000å±€...")
    trainer.train(num_episodes=1000, opponent_types=["humanlike", "simple", "simple"])
    
    print("\nè¯„ä¼°...")
    results = trainer.evaluate(num_games=100, opponent_types=["humanlike", "simple", "simple"])
    print(f"æœ€ç»ˆèƒœç‡: {results['win_rate']:.2%}")


def advanced_config():
    """é«˜çº§é…ç½®ï¼šæœ€ä¼˜è¶…å‚æ•°"""
    print("âš™ï¸ é«˜çº§é…ç½®è®­ç»ƒ")
    
    config = {
        "learning_rate": 0.0005,      # è¾ƒå°çš„å­¦ä¹ ç‡
        "discount_factor": 0.97,       # æ›´é‡è§†é•¿æœŸå¥–åŠ±
        "epsilon": 1.0,
        "epsilon_decay": 0.9997,       # éå¸¸æ…¢çš„è¡°å‡
        "epsilon_min": 0.03,           # ä¿æŒ3%æ¢ç´¢
        "batch_size": 128,             # æ›´å¤§çš„batch
        "warmup_steps": 1000,          # æ›´å¤šwarmup
        "buffer_size": 30000,          # æ›´å¤§çš„buffer
    }
    
    trainer = AdvancedRLTrainer("dqn", config, "advanced_config")
    trainer.train(
        num_episodes=10000,
        opponent_types=["humanlike", "humanlike", "simple"],  # æ›´å¼ºçš„å¯¹æ‰‹
        eval_interval=200,
        eval_games=50,
        save_interval=1000,
        plot_interval=200,
        shuffle_positions=True  # å¯ç”¨ä½ç½®æ‰“ä¹±
    )


def position_robustness_test():
    """ä½ç½®é²æ£’æ€§å¯¹æ¯”å®éªŒ"""
    print("ğŸ”„ ä½ç½®é²æ£’æ€§æµ‹è¯•")
    
    config = {
        "learning_rate": 0.001,
        "discount_factor": 0.95,
        "epsilon": 1.0,
        "epsilon_decay": 0.998,
        "epsilon_min": 0.05,
    }
    
    # å®éªŒ1ï¼šä¸æ‰“ä¹±ä½ç½®
    print("\nå®éªŒ1: å›ºå®šä½ç½®è®­ç»ƒ...")
    trainer_fixed = AdvancedRLTrainer("dqn", config, "position_test_fixed")
    trainer_fixed.train(
        num_episodes=400,
        opponent_types=["humanlike", "smarter", "smarter"],
        eval_interval=50,
        eval_games=20,
        save_interval=500,
        plot_interval=50,
        shuffle_positions=False  # å…³é”®ï¼šä¸æ‰“ä¹±
    )
    
    # å®éªŒ2ï¼šæ‰“ä¹±ä½ç½®
    print("\nå®éªŒ2: éšæœºä½ç½®è®­ç»ƒ...")
    trainer_shuffled = AdvancedRLTrainer("dqn", config, "position_test_shuffled")
    trainer_shuffled.train(
        num_episodes=400,
        opponent_types=["humanlike", "smarter", "smarter"],
        eval_interval=50,
        eval_games=20,
        save_interval=500,
        plot_interval=50,
        shuffle_positions=True  # å…³é”®ï¼šæ‰“ä¹±
    )
    
    print("\nå¯¹æ¯”ç»“æœï¼š")
    print("  æŸ¥çœ‹ experiments/position_test_fixed/plots/")
    print("  æŸ¥çœ‹ experiments/position_test_shuffled/plots/")
    print("  é¢„æœŸï¼šæ‰“ä¹±ä½ç½®çš„æ¨¡å‹åœ¨ä¸åŒä½ç½®ä¸‹è¡¨ç°æ›´ç¨³å®š")


if __name__ == "__main__":
    import sys
    
    configs = {
        "quick": quick_test,
        "medium": medium_training,
        "long": long_training,
        "baseline": baseline_comparison,
        "advanced": advanced_config,
        "position": position_robustness_test,  # æ–°å¢
    }
    
    if len(sys.argv) > 1 and sys.argv[1] in configs:
        configs[sys.argv[1]]()
    else:
        print("\nå¯ç”¨çš„è®­ç»ƒé…ç½®:")
        print("  python train_configs.py quick     - å¿«é€Ÿæµ‹è¯•ï¼ˆ100å±€ï¼‰")
        print("  python train_configs.py medium    - ä¸­ç­‰è§„æ¨¡ï¼ˆ1000å±€ï¼‰")
        print("  python train_configs.py long      - é•¿æ—¶é—´è®­ç»ƒï¼ˆ5000å±€ï¼‰")
        print("  python train_configs.py baseline  - åŸºçº¿å¯¹æ¯”")
        print("  python train_configs.py advanced  - é«˜çº§é…ç½®ï¼ˆ10000å±€ï¼‰")
        print("  python train_configs.py position  - ä½ç½®é²æ£’æ€§å¯¹æ¯”å®éªŒ")
        print("\né»˜è®¤è¿è¡Œä¸­ç­‰è§„æ¨¡è®­ç»ƒ...")
        print("  python train_configs.py advanced  - é«˜çº§é…ç½®ï¼ˆ10000å±€ï¼‰")
        print("\né»˜è®¤è¿è¡Œä¸­ç­‰è§„æ¨¡è®­ç»ƒ...")
        medium_training()
