1. rl_agent.py 中 load_model 函数没有被调用过，可能是因为目前并没有要用它的地方。
在train的main中： trainer = RLTrainer(agent_config)创建了一个agent实例，q表和epsilon值存在trainer的成员变量中，
train时不断更新q_table和epsilon，evaluate时也不需要重新加载qtable，而是直接调用traine的成员变量，epsilon则是通过
istraining=false直接使其失效

2. player.py 的reward计算函数 reflect 中：是否应该用“你”来判断玩家类别


